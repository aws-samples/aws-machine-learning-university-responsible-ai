{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"0\">Responsible AI - Equalized Odds</a>\n",
    "\n",
    "This notebook shows how to process outputs produced by a probabilistic model to generate fairer results. We will use a logistic regression model to predict whether an individuals' income is $\\leq$ 50k or not using US census data.\n",
    "\n",
    "__Dataset:__ \n",
    "The dataset we will use for this exercise is coming from [folktables](https://github.com/zykls/folktables). Folktables provide code to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files managed by the US Census Bureau. The data itself is governed by the terms of use provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html).\n",
    "\n",
    "__ML Problem:__ \n",
    "Ultimately, the goal will be to predict whether an individual's income is above \\\\$50,000. We will filter the ACS PUMS data sample to only include individuals above the age of 16, who reported usual working hours of at least 1 hour per week in the past year, and an income of at least \\\\$100. The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable substitute to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult). The income threshold can be changed easily to define new prediction tasks.\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Data Processing</a>\n",
    "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
    "    * <a href=\"#22\">Select features to build the model</a>\n",
    "    * <a href=\"#23\">Feature Transformation</a>\n",
    "    * <a href=\"#24\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#25\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "3. <a href=\"#3\">Train (and Tune) a Classifier</a>\n",
    "4. <a href=\"#4\">Test the Classifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes an installation of the SageMaker kernel `.conda-mlu-rai:Python` through the `environment.yaml` file in SageMaker Sudio Labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Reshaping/basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Operational libraries\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "# Fairness libraries\n",
    "from folktables.acs import *\n",
    "from folktables.folktables import *\n",
    "from folktables.load_acs import *\n",
    "from fairlearn.reductions import EqualizedOdds\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "# Jupyter(lab) libraries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To read in the dataset, we will be using [folktables](https://github.com/zykls/folktables) which provides access to the US Census dataset. Folktables contains predefined prediction tasks but also allows the user to specify the problem type.\n",
    "\n",
    "The US Census dataset distinguishes between household and individuals. To obtain data on individuals, we use `ACSDataSource` with `survey=person`. The feature names for the US Census data follow the same distinction and use `P` for `person` and `H` for `household`, e.g.: `AGEP` refers to age of an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_features = [\n",
    "    \"AGEP\",  # age individual\n",
    "    \"COW\",  # class of worker\n",
    "    \"SCHL\",  # educational attainment\n",
    "    \"MAR\",  # marital status\n",
    "    \"OCCP\",  # occupation\n",
    "    \"POBP\",  # place of birth\n",
    "    \"RELP\",  # relationship\n",
    "    \"WKHP\",  # hours worked per week past 12 months\n",
    "    \"SEX\",  # sex\n",
    "    \"RAC1P\",  # recorded detailed race code\n",
    "    \"PWGTP\",  # persons weight\n",
    "    \"GCL\",  # grand parents living with grandchildren\n",
    "]\n",
    "\n",
    "# Define the prediction problem and features\n",
    "ACSIncome = folktables.BasicProblem(\n",
    "    features=income_features,\n",
    "    target=\"PINCP\",  # total persons income\n",
    "    target_transform=lambda x: x > 50000,\n",
    "    group=\"RAC1P\",\n",
    "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    postprocess=lambda x: x,  # applies post processing, e.g. fill all NAs\n",
    ")\n",
    "\n",
    "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "# Specify region (here: California) and load data\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "# Apply transformation as per problem statement above\n",
    "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)\n",
    "\n",
    "# Convert numpy array to dataframe\n",
    "df = pd.DataFrame(\n",
    "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
    "    columns=income_features + [\">50k\"],\n",
    ")\n",
    "\n",
    "# For further modelling you want to use only 2 groups\n",
    "df = df[df[\"RAC1P\"].isin([6, 8])].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. <a name=\"2\">Data Processing</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We look at number of rows, columns, and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check how many rows and columns we have in the data frame\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that all columns are numerical (`dtype = float64`). However, when checking the column headers (and information at top of the notebook), we should notice that we are actually dealing with multimodal data. We expect to see a mix of categorical, numerical and potentially even text information.\n",
    "\n",
    "Let's cast the features accordingly. We start by creating list for each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"COW\",\n",
    "    \"SCHL\",\n",
    "    \"MAR\",\n",
    "    \"OCCP\",\n",
    "    \"POBP\",\n",
    "    \"RELP\",\n",
    "    \"SEX\",\n",
    "    \"GCL\",\n",
    "]\n",
    "\n",
    "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cast categorical features to `category`\n",
    "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
    "\n",
    "# Cast numerical features to `int`\n",
    "df[numerical_features] = df[numerical_features].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with `.info()` again to make sure the changes took effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, so we can now separate model features from model target and sensitive feature to explore them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensitive_feature = \"RAC1P\"\n",
    "\n",
    "model_target = \">50k\"\n",
    "model_features = categorical_features + numerical_features\n",
    "\n",
    "print(\"Model features: \", model_features)\n",
    "print(\"Model target: \", model_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check that that target is not accidentally part of the features\n",
    "model_target in model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good here. We made sure that the target is not in the feature list. If we find the above statement showing `True` we need to remove the target by calling `model_features.remove(model_target)`.\n",
    "\n",
    "Let's have a look at missing values next.\n",
    "\n",
    "\n",
    "#### Missing values\n",
    "The quickest way to check for missing values is to use `.isna().sum()`. This will provide a count of how many missing values we have. In fact, we can also see the count of missing values with `.info()` as it provided a count of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the plots, let's have a look at how many unique instances we have per column. This helps us avoid plotting charts with hundreds of unique values. Let's filter for columns with fewer than 10 unique instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortlist_fts = (\n",
    "    df[model_features]\n",
    "    .apply(lambda col: col.nunique())\n",
    "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(shortlist_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[model_target].value_counts().plot.bar(color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we are dealing with an imbalanced dataset. This means there are more examples for one type of results (here: 0; meaning individuals earning $\\leq$ 50k). This is relevant for model choice and potential up-sampling or down-sampling to balance out the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution(s)\n",
    "\n",
    "Let's now plot bar charts for the shortlist features of our dataset (as per above: shortlist - feature columns with less than 10 unique instance classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "fig.suptitle(\"Feature Bar Plots\")\n",
    "\n",
    "fts = range(len(shortlist_fts.index.tolist()))\n",
    "for i, ax in zip(fts, axs.ravel()):\n",
    "    df[shortlist_fts.index.tolist()[i]].value_counts().plot.bar(color=\"black\", ax=ax)\n",
    "    ax.set_title(shortlist_fts.index.tolist()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <a name=\"22\">Select features to build the model</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "During the extended EDA in the DATAPREP notebook, we learned that `GCL` is a feature that is equally present for both outcome types and also contains a lot of missing values. Therefore, we can drop it from the list of features we want to use for model build. We also drop `OCCP` and `POBP` as those features have too many unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_remove = [\"GCL\", \"OCCP\", \"POBP\"]\n",
    "\n",
    "# Drop to_remove features from the respective list(s) - if applicable\n",
    "for ft in to_remove:\n",
    "    if ft in model_features:\n",
    "        model_features.remove(ft)\n",
    "    if ft in categorical_features:\n",
    "        categorical_features.remove(ft)\n",
    "    if ft in numerical_features:\n",
    "        numerical_features.remove(ft)\n",
    "\n",
    "# Let's also clean up the dataframe and only keep the features and columns we need\n",
    "df = df[model_features + [sensitive_feature] + [model_target]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 <a name=\"23\">Feature transformation</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "In this notebook, we won't perform any transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 <a name=\"23\">Train - Validation - Test Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "To get a training, test and validation set, we will use sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    df, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Print the shapes of the Train - Test Datasets\n",
    "print(\n",
    "    \"Train - Test - Validation datasets shapes: \",\n",
    "    train_data.shape,\n",
    "    test_data.shape,\n",
    "    val_data.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 <a name=\"24\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "Let's build a full model pipeline. We need preprocessing split per data type, and then combine everything back into a composite pipeline along with a model. To achieve this, we will use sklearns `Pipeline` and `ColumnTransformer`.\n",
    "\n",
    "__Step 1 (set up pre-processing per data type):__\n",
    "> For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's `SimpleImputer`, followed by a `MinMaxScaler` (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "\n",
    " > In the categorical features pipeline, the __categorical_processor__ below, we impute with a placeholder value and encode with sklearn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    " \n",
    "__Step 2 (combining pre-processing methods into a transformer):__ \n",
    " > The selective preparations of the dataset features are then put together into a collective `ColumnTransformer`, to be finally used in a Pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future.\n",
    "   \n",
    "__Step 3 (combining transformer with a model):__ \n",
    "> Combine `ColumnTransformer` from Step 2 with a selected algorithm in a new pipeline. For example, the algorithm could be a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### STEP 1 ###\n",
    "##############\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [(\"num_imputer\", SimpleImputer(strategy=\"mean\")), (\"num_scaler\", MinMaxScaler())]\n",
    ")\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### STEP 2 ###\n",
    "##############\n",
    "\n",
    "# Combine all data preprocessors from above\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "### STEP 3 ###\n",
    "##############\n",
    "\n",
    "# Pipeline desired all data transformers, along with an estimator at the end\n",
    "# Later you can set/reach the parameters using the names issued - for hyperparameter tuning, for example\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_processing\", data_processor),\n",
    "        (\"lg\", LogisticRegression(random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will come in handy especially when building more complex pipelines, stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train a Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use the pipeline, along with a Logistic Regression estimator for training and Equalized Odds postprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We train the classifier with __.fit()__ on our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get train data to train the classifier\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the classifier to the train data\n",
    "# Train data going through the Pipeline is imputed (with means from the train data),\n",
    "#   scaled (with the min/max from the train data),\n",
    "#   and finally used to fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to enforce Equalized Odds. To do so, we are going to use fairlearns `ThresholdOptimzer` to implement Equality of Opportunity. It is a postprocessing algorithm based on the paper [Equality of Opportunity in Supervised Learning](https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf). This technique takes  an existing classifier and the sensitive feature as inputs, and derives a transformation of the classifier's prediction to enforce the specified parity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up ThresholdOptimizer\n",
    "eo_model = ThresholdOptimizer(\n",
    "    estimator=pipeline[-1],\n",
    "    constraints=\"equalized_odds\",\n",
    "    objective=\"accuracy_score\",\n",
    "    grid_size=1000,\n",
    "    flip=False,\n",
    "    prefit=False,\n",
    "    predict_method=\"deprecated\",\n",
    ")\n",
    "\n",
    "# Learn the transformation & extract feature names\n",
    "data_processor.fit(X_train)\n",
    "\n",
    "# To extract feature names we first need to fit the data processor as this will generate the one hot encoding\n",
    "ft_names = numerical_features + list(\n",
    "    data_processor.transformers_[1][1]\n",
    "    .named_steps[\"cat_encoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "# Add column names and convert to data frame\n",
    "X_train_prep = pd.DataFrame(\n",
    "        data_processor.transform(X_train).todense(), columns=ft_names\n",
    "    )\n",
    "\n",
    "# Adjust the results that the classifier would produce by letting ThresholdOptimizer know what the sensitive features are\n",
    "eo_model.fit(X_train_prep, y_train, sensitive_features=train_data[\"RAC1P\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can now use the fitted equalized odds post-processor and use it to create adjusted outcomes (predictions)\n",
    "y_train_adjusted = eo_model.predict(\n",
    "    X_train_prep, sensitive_features=train_data[\"RAC1P\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the data, adjusted predictions, original predictions, and the true outcome\n",
    "eop_df_train = pd.DataFrame(\n",
    "    {\n",
    "        \"RAC1P\": train_data[\"RAC1P\"].reset_index(drop=True),\n",
    "        \"y_train_adjusted\": y_train_adjusted,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"y_train_true\": train_data[model_target].reset_index(drop=True),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Test the Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's now evaluate the performance of the trained classifier on the test dataset. We use __.predict()__ this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the adjusted predictions for test and convert to dataframe\n",
    "y_test_adjusted = eo_model.predict(\n",
    "    np.asarray(data_processor.transform(test_data[model_features]).todense()),\n",
    "    sensitive_features=test_data[\"RAC1P\"].values,\n",
    ")\n",
    "\n",
    "# Let's have a look at the adjusted outputs for the test dataset\n",
    "print(y_test_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the data, adjusted predictions, original predictions, and the true outcome\n",
    "eop_df_test = pd.DataFrame(\n",
    "    {\n",
    "        \"RAC1P\": test_data[\"RAC1P\"].reset_index(drop=True),\n",
    "        \"y_test_adjusted\": y_test_adjusted,\n",
    "        \"y_test_true\": test_data[model_target].reset_index(drop=True),\n",
    "        \"y_test_withoutEO\": pipeline.predict(test_data[model_features]),\n",
    "    }\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Model Predictions and Baseline\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Baseline target distribution\")\n",
    "ax2.title.set_text(\"Prediction EO adjusted\")\n",
    "ax3.title.set_text(\"Prediction without EO adjustment\")\n",
    "\n",
    "# Create plots\n",
    "eop_df_test.groupby([\"RAC1P\", \"y_test_true\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax1\n",
    ")\n",
    "eop_df_test.groupby([\"RAC1P\", \"y_test_adjusted\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax2\n",
    ")\n",
    "eop_df_test.groupby([\"RAC1P\", \"y_test_withoutEO\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax3\n",
    ")\n",
    "# Align y-axis\n",
    "ax2.sharey(ax1)\n",
    "ax3.sharey(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that the ratios between the groups were adjusted drastically (compared to predictions without EO postprocessing). The question remains whether it was fair the reduce the number of positive outcomes for RAC1P group 6. The odds are equal now, but this came at the cost of placing a lot of outcomes that were actually positive into the negative bucket. You can see that if you hadn't enforced Equalized Odds, the model would have actually amplified the existing bias in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-mlu-rai:Python",
   "language": "python",
   "name": "conda-env-.conda-mlu-rai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
