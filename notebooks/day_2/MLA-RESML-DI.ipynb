{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"0\">Responsible AI - Disparate Impact</a>\n",
    "\n",
    "This notebook shows how to quantify disparate impact and covers the implementation of a basic disparate impact remover. We will use a Logistic Regression classifier to predict whether an individuals' income is $\\leq$ 50k or not using US census data.\n",
    "\n",
    "__Dataset:__ \n",
    "You will download a dataset for this exercise using [folktables](https://github.com/zykls/folktables). Folktables provides an API to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files which are managed by the US Census Bureau. The data itself is governed by the terms of use provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html).\n",
    "\n",
    "__ML Problem:__ \n",
    "Ultimately, the goal will be to predict whether an individual's income is above \\\\$50,000. We will filter the ACS PUMS data sample to only include individuals above the age of 16, who reported usual working hours of at least 1 hour per week in the past year, and an income of at least \\\\$100. The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable substitute to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult). The income threshold can be changed easily to define new prediction tasks.\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Data Processing</a>\n",
    "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
    "    * <a href=\"#22\">Select features to build the model</a>\n",
    "    * <a href=\"#23\">Feature Transformation</a>\n",
    "    * <a href=\"#24\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#25\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "3. <a href=\"#3\">Train (and Tune) a Classifier</a>\n",
    "4. <a href=\"#4\">Test the Classifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes an installation of the SageMaker kernel `.conda-mlu-rai:Python` through the `environment.yaml` file in SageMaker Sudio Labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Reshaping/basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Operational libraries\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Fairness libraries\n",
    "from folktables.acs import *\n",
    "from folktables.folktables import *\n",
    "from folktables.load_acs import *\n",
    "from aif360.datasets import BinaryLabelDataset, Dataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "# Jupyter(lab) libraries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To read in the dataset, we will be using [folktables](https://github.com/zykls/folktables) which provides access to the US Census dataset. Folktables contains predefined prediction tasks but also allows the user to specify the problem type.\n",
    "\n",
    "The US Census dataset distinguishes between household and individuals. To obtain data on individuals, we use `ACSDataSource` with `survey=person`. The feature names for the US Census data follow the same distinction and use `P` for `person` and `H` for `household`, e.g.: `AGEP` refers to age of an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_features = [\n",
    "    \"AGEP\",  # age individual\n",
    "    \"COW\",  # class of worker\n",
    "    \"SCHL\",  # educational attainment\n",
    "    \"MAR\",  # marital status\n",
    "    \"OCCP\",  # occupation\n",
    "    \"POBP\",  # place of birth\n",
    "    \"RELP\",  # relationship\n",
    "    \"WKHP\",  # hours worked per week past 12 months\n",
    "    \"SEX\",  # sex\n",
    "    \"RAC1P\",  # recorded detailed race code\n",
    "    \"PWGTP\",  # persons weight\n",
    "    \"GCL\",  # grand parents living with grandchildren\n",
    "    \"SCH\",  # school enrollment\n",
    "]\n",
    "\n",
    "# Define the prediction problem and features\n",
    "ACSIncome = folktables.BasicProblem(\n",
    "    features=income_features,\n",
    "    target=\"PINCP\",  # total persons income\n",
    "    target_transform=lambda x: x > 50000,\n",
    "    group=\"RAC1P\",\n",
    "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    postprocess=lambda x: x,  # applies post processing, e.g. fill all NAs\n",
    ")\n",
    "\n",
    "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "# Specify region (here: California) and load data\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "# Apply transformation as per problem statement above\n",
    "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)\n",
    "\n",
    "# Convert numpy array to dataframe\n",
    "df = pd.DataFrame(\n",
    "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
    "    columns=income_features + [\">50k\"],\n",
    ")\n",
    "\n",
    "# For further modelling you want to use only 2 groups\n",
    "df = df[df[\"RAC1P\"].isin([6, 8])].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. <a name=\"2\">Data Processing</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We look at number of rows, columns, and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check how many rows and columns we have in the data frame\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that all columns are numerical (`dtype = float64`). However, when checking the column headers (and information at top of the notebook), we should notice that we are actually dealing with multimodal data. We expect to see a mix of categorical, numerical and potentially even text information.\n",
    "\n",
    "Let's cast the features accordingly. We start by creating list for each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"COW\",\n",
    "    \"SCHL\",\n",
    "    \"MAR\",\n",
    "    \"OCCP\",\n",
    "    \"POBP\",\n",
    "    \"RELP\",\n",
    "    \"SEX\",\n",
    "    \"GCL\",\n",
    "    \"SCH\",\n",
    "]\n",
    "\n",
    "sensitive_attribute = \"RAC1P\"\n",
    "\n",
    "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We cast categorical features to `category`\n",
    "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
    "\n",
    "# We cast the sensitive attribute as `category`\n",
    "df[sensitive_attribute] = df[sensitive_attribute].astype(\"object\")\n",
    "\n",
    "# We cast numerical features to `int`\n",
    "df[numerical_features] = df[numerical_features].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with `.info()` again to make sure the changes took effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, so we can now separate model features from model target to explore them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_target = \">50k\"\n",
    "model_features = categorical_features + numerical_features + [sensitive_attribute]\n",
    "\n",
    "print(\"Model features: \", model_features)\n",
    "print(\"Model target: \", model_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check that that target is not accidentally part of the features\n",
    "model_target in model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good here. We made sure that the target is not in the feature list. If we find the above statement showing `True` we need to remove the target by calling `model_features.remove(model_target)`.\n",
    "\n",
    "Let's have a look at missing values next.\n",
    "\n",
    "\n",
    "#### Missing values\n",
    "The quickest way to check for missing values is to use `.isna().sum()`. This will provide a count of how many missing values we have. In fact, we can also see the count of missing values with `.info()` as it provided a count of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the plots, let's have a look at how many unique instances we have per column. This helps us avoid plotting charts with hundreds of unique values. Let's filter for columns with fewer than 10 unique instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortlist_fts = (\n",
    "    df[model_features]\n",
    "    .apply(lambda col: col.nunique())\n",
    "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(shortlist_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[model_target].value_counts().plot.bar(color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we are dealing with an imbalanced dataset. This means there are more examples for one type of results (here: 0; meaning individuals earning $\\leq$ 50k). This is relevant for model choice and potential up-sampling or down-sampling to balance out the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution(s)\n",
    "\n",
    "Let's now plot bar charts for the shortlist features of our dataset (as per above: shortlist - feature columns with less than 10 unique instance classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "fig.suptitle(\"Feature Bar Plots\")\n",
    "\n",
    "fts = range(len(shortlist_fts.index.tolist()))\n",
    "for i, ax in zip(fts, axs.ravel()):\n",
    "    df[shortlist_fts.index.tolist()[i]].value_counts().plot.bar(color=\"black\", ax=ax)\n",
    "    ax.set_title(shortlist_fts.index.tolist()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 <a name=\"22\">Select features to build the model</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "During the extended EDA in the DATAPREP notebook, we learned that `GCL` is a feature that is equally present for both outcome types and also contains a lot of missing values. Therefore, we can drop it from the list of features we want to use for model build. We also drop `OCCP` and `POBP` as those features have too many unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_remove = [\"GCL\", \"OCCP\", \"POBP\"]\n",
    "\n",
    "# Drop to_remove features from the respective list(s) - if applicable\n",
    "for ft in to_remove:\n",
    "    if ft in model_features:\n",
    "        model_features.remove(ft)\n",
    "    if ft in categorical_features:\n",
    "        categorical_features.remove(ft)\n",
    "    if ft in numerical_features:\n",
    "        numerical_features.remove(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's also clean up the dataframe and only keep the features and columns we need\n",
    "df = df[model_features + [model_target]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 <a name=\"23\">Feature transformation</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "In the paper [Certifying and Removing Disparate Impact](https://arxiv.org/pdf/1412.3756.pdf) a definition for Disparate Impact was introduced as the ratio of probability of positive outcomes for the disfavored group $(A=0)$ to probability of positive outcomes for the favored group $(A=1)$:  \n",
    "\n",
    "$\\large DI = \\frac{Pr(Y=y|A=0)}{Pr(Y=y|A=1)}$\n",
    "    \n",
    "\n",
    "To be 'disparate impact free' the value calculated with the above equation needs to be $>$0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Dataset construct for AIF360\n",
    "binaryLabelDataset = BinaryLabelDataset(\n",
    "    df=df,\n",
    "    label_names=[\">50k\"],\n",
    "    protected_attribute_names=[\"RAC1P\"],\n",
    "    favorable_label=1.0,\n",
    "    unfavorable_label=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to declare what the attribute value of the (un)privileged group is\n",
    "priv_group = [{\"RAC1P\": 6}]\n",
    "unpriv_group = [{\"RAC1P\": 8}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the disparate impact (DI) value with the __AIF360__ inbuilt method `BinaryLabelDatasetMetric()`. You need to pass the data frame into the metric and then you can calculate DI by using `.disparate_impact()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the DI value\n",
    "print(\n",
    "    BinaryLabelDatasetMetric(\n",
    "        binaryLabelDataset,\n",
    "        unprivileged_groups=unpriv_group,\n",
    "        privileged_groups=priv_group,\n",
    "    ).disparate_impact()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 <a name=\"24\">Train - Validation - Test Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "To get a training, test and validation set, we will use sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    df, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Print the shapes of the Train - Test Datasets\n",
    "print(\n",
    "    \"Train - Test - Validation datasets shapes: \",\n",
    "    train_data.shape,\n",
    "    test_data.shape,\n",
    "    val_data.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 <a name=\"25\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "Let's build a data processing pipeline. We need pre-processing split per data type, and then combine everything back into a composite pipeline. To achieve this, we will use sklearns `Pipeline` and `ColumnTransformer`.\n",
    "\n",
    "__Step 1 (set up pre-processing per data type):__\n",
    "> For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's `SimpleImputer`, followed by a `MinMaxScaler` (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "\n",
    " > In the categorical features pipeline, the __categorical_processor__ below, we impute with a placeholder value and encode with sklearn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    " \n",
    "__Step 2 (combining pre-processing methods into a transformer):__ \n",
    " > The selective preparations of the dataset features are then put together into a collective `ColumnTransformer`, to be finally used in a Pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### STEP 1 ###\n",
    "##############\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [(\"num_imputer\", SimpleImputer(strategy=\"mean\")), (\"num_scaler\", MinMaxScaler())]\n",
    ")\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### STEP 2 ###\n",
    "##############\n",
    "\n",
    "# Combine all data preprocessors from above\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, we want to use the data processing pipeline to prepare the data before we perform the DI removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learn the transformation & extract feature names\n",
    "data_processor.fit(train_data)\n",
    "\n",
    "# To extract feature names we first need to fit the data processor as this will generate the one hot encoding\n",
    "ft_names = numerical_features + list(\n",
    "    data_processor.transformers_[1][1]\n",
    "    .named_steps[\"cat_encoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_prep(data, fitted_transformation, feature_names):\n",
    "    \"\"\"\n",
    "    Function to create an AIF360 dataset based on processed data\n",
    "    \"\"\"\n",
    "    # We join the encoded data with the target and sensitive attribute to create a new AIF360 dataset\n",
    "    prep = np.concatenate(\n",
    "        (\n",
    "            fitted_transformation.transform(data).todense(),\n",
    "            data[[model_target]].values,\n",
    "            data[[sensitive_attribute]].values,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Add column names and convert to data frame\n",
    "    prep_df = pd.DataFrame(\n",
    "        prep, columns=feature_names + [model_target] + [sensitive_attribute]\n",
    "    )\n",
    "\n",
    "    # Create a Dataset construct for AIF360 for training data\n",
    "    df_aif360 = BinaryLabelDataset(\n",
    "        df=prep_df,\n",
    "        label_names=[model_target],\n",
    "        protected_attribute_names=[sensitive_attribute],\n",
    "        favorable_label=1.0,\n",
    "        unfavorable_label=0.0,\n",
    "    )\n",
    "    return df_aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use function to create AIF360 datasets\n",
    "train_aif360 = data_prep(train_data, data_processor, ft_names)\n",
    "val_aif360 = data_prep(val_data, data_processor, ft_names)\n",
    "test_aif360 = data_prep(test_data, data_processor, ft_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find out which index the sensitive attribute has to delete later\n",
    "sensitive_attribute_index = train_aif360.feature_names.index(sensitive_attribute)\n",
    "\n",
    "# Initialize DI remover\n",
    "di = DisparateImpactRemover(repair_level=0.9)\n",
    "\n",
    "# Perform repair\n",
    "train_repd = di.fit_transform(train_aif360)\n",
    "\n",
    "# Convert to dataframe\n",
    "df_transform_train = train_repd.convert_to_dataframe()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To visualize the change, you can plot the distribution of one of the numerical features from the training dataset before and after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Feature before and after Transformation\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Normal (scaled) distribution\")\n",
    "ax2.title.set_text(\"Transformed distribution\")\n",
    "\n",
    "# Create plots\n",
    "sns.histplot(train_aif360.convert_to_dataframe()[0], x=\"AGEP\", hue=\"RAC1P\", ax=ax1)\n",
    "sns.histplot(df_transform_train, x=\"AGEP\", hue=\"RAC1P\", ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. <a name=\"3\">Train (and Tune) a Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use a Logistic Regression estimator and the transformed data for training. Then we will try to find the best possible repair level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Training\n",
    "\n",
    "Get the correct training data and corresponding labels. Make sure to delete the sensitive feature before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features (and delete sensitive attribute)\n",
    "X_train = np.delete(train_repd.features, sensitive_attribute_index, axis=1)\n",
    "\n",
    "# Extract label\n",
    "y_train = train_repd.labels.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the classifier with `.fit()` on our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr = LogisticRegression(solver=\"lbfgs\", penalty=None)\n",
    "\n",
    "# Train model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Model performance on the training set:\")\n",
    "print(\"Training accuracy:\", accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create copy of training dataset and save over the labels with the new predictions\n",
    "train_repd_pred = train_repd.copy()\n",
    "train_repd_pred.labels = lr.predict(X_train)\n",
    "\n",
    "# Create metric\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    train_repd_pred, privileged_groups=priv_group, unprivileged_groups=unpriv_group\n",
    ").disparate_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Model fairness on the training set:\")\n",
    "print(\"Training DI:\", metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fairness Tuning\n",
    "\n",
    "Clearly this is worse than where we started before the transformation. Let's see if we can find the best possible repair level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create list to store DI values at different repair levels\n",
    "DIs = []\n",
    "\n",
    "# Loop over different repair levels\n",
    "for level in tqdm(np.arange(0.0, 1.1, 0.1)):\n",
    "    # Initialize DI remover\n",
    "    di = DisparateImpactRemover(repair_level=level)\n",
    "\n",
    "    # Perform repair\n",
    "    train_repd = di.fit_transform(train_aif360)\n",
    "    val_repd = di.fit_transform(val_aif360)\n",
    "\n",
    "    # Extract features (and delete sensitive attribute)\n",
    "    X_tr = np.delete(train_repd.features, sensitive_attribute_index, axis=1)\n",
    "    X_va = np.delete(val_repd.features, sensitive_attribute_index, axis=1)\n",
    "\n",
    "    # Extract label\n",
    "    y_tr = train_repd.labels.ravel()\n",
    "    y_va = val_repd.labels.ravel()\n",
    "\n",
    "    # Initialize model\n",
    "    lr = LogisticRegression(solver=\"lbfgs\", penalty=None)\n",
    "\n",
    "    # Train model\n",
    "    lr.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predict with model\n",
    "    val_repd_pred = val_repd.copy()\n",
    "    val_repd_pred.labels = lr.predict(X_va)\n",
    "\n",
    "    # Create metric\n",
    "    metric = BinaryLabelDatasetMetric(\n",
    "        val_repd_pred, privileged_groups=priv_group, unprivileged_groups=unpriv_group\n",
    "    )\n",
    "    # Append DI value\n",
    "    DIs.append(metric.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the repair level and corresponding DI on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.arange(0.0, 1.1, 0.1), DIs, marker=\"o\")\n",
    "plt.plot([0, 1], [1, 1], \"g\")\n",
    "plt.plot([0, 1], [0.8, 0.8], \"r\")\n",
    "plt.ylim([0.2, 1.2])\n",
    "plt.ylabel(\"Disparate Impact (DI)\")\n",
    "plt.xlabel(\"repair level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Test the Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's now evaluate the performance and fairness of the trained classifier on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the test dataset\n",
    "test_repd = di.fit_transform(test_aif360)\n",
    "\n",
    "# Extract features and labels from AIF360 dataset construct\n",
    "X_te = np.delete(test_repd.features, sensitive_attribute_index, axis=1)\n",
    "y_te = test_repd.labels.ravel()\n",
    "\n",
    "# Use the last (and most fair) fitted model to make predictions on the test dataset\n",
    "y_te_pred = lr.predict(X_te)\n",
    "\n",
    "print(\"Model performance on the test set:\")\n",
    "print(\"Test accuracy:\", accuracy_score(y_te, y_te_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous notebook, we might remember that the accuracy was around 80%, so pretty much the same as we see above. So far so good, but let's check if the outcomes are fairer on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict with model\n",
    "test_repd_pred = test_repd.copy()\n",
    "test_repd_pred.labels = y_te_pred\n",
    "\n",
    "# Create metric\n",
    "BinaryLabelDatasetMetric(\n",
    "    test_repd_pred, privileged_groups=priv_group, unprivileged_groups=unpriv_group\n",
    ").disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much lower than the values we had originally (before the transformation when DI was around 0.4). So we are in fact even further away from not having DI impact. Let's wrap up by visualizing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataframe that contains predictions and the sensitive attribute\n",
    "di_df = pd.concat(\n",
    "    [\n",
    "        test_data.reset_index(drop=True)[[\"RAC1P\", \">50k\"]],\n",
    "        pd.Series(y_te_pred, name=\"y_test_pred\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Model Predictions and Baseline\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Baseline target distribution\")\n",
    "ax2.title.set_text(\"Predictions with bias mitigation\")\n",
    "\n",
    "# Create plots\n",
    "di_df.groupby([\"RAC1P\", model_target]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax1\n",
    ")\n",
    "di_df.groupby([\"RAC1P\", \"y_test_pred\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax2\n",
    ")\n",
    "# Align y-axis\n",
    "ax2.sharey(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the results for the disfavored group have worsened. This could be explained by the fact that we are dealing with an imbalance in the class label itself and outcome '0' will be favored by the model in general. It appears that group 6 does not gain much advantage despite the DI transformation. As final check, we can look at model predictions for the dataset without DI transformation and compare what would have happened if we hadn't intervened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create predictions for the test data without DI transformation\n",
    "lr.fit(data_processor.transform(train_data), train_data[model_target])\n",
    "noDI_test_preds = lr.predict(data_processor.transform(test_data))\n",
    "\n",
    "# Join the predictions to the dataframe we need for plotting\n",
    "di_df = pd.concat(\n",
    "    [\n",
    "        di_df,\n",
    "        pd.Series(noDI_test_preds, name=\"y_test_pred_noDI\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Model Predictions and Baseline\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Baseline target distribution\")\n",
    "ax2.title.set_text(\"Predictions with bias mitigation\")\n",
    "ax3.title.set_text(\"Predictions without bias mitigation (DI transformation)\")\n",
    "\n",
    "# Create plots\n",
    "di_df.groupby([\"RAC1P\", model_target]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax1\n",
    ")\n",
    "di_df.groupby([\"RAC1P\", \"y_test_pred\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax2\n",
    ")\n",
    "di_df.groupby([\"RAC1P\", \"y_test_pred_noDI\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax3\n",
    ")\n",
    "\n",
    "# Align y-axis\n",
    "ax2.sharey(ax1)\n",
    "ax3.sharey(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, in the model that was trained without DI remover, group 6 had an even higher number of predictions for the < 50k class and was therefor more unfair. For group 8 we notice that there are slightly fewer positive outcomes when DI transformation is considered. Generally, what we should notice is that while a model might have good accuracy, we still need to check outcome distributions per target class as errors might be disproportionately concentrated in a certain group. For this particular dataset DI removal did not have the desired effect and we should look into other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-mlu-rai:Python",
   "language": "python",
   "name": "conda-env-.conda-mlu-rai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
