{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a name=\"0\">Responsible AI - Logistic Regression </a>\n",
    "\n",
    "\n",
    "This notebook shows how to build a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model to predict whether an individuals' income is $\\leq$ 50k or not using US census data.\n",
    "\n",
    "__Dataset:__ \n",
    "You will download a dataset for this exercise using [folktables](https://github.com/zykls/folktables). Folktables provides an API to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files which are managed by the US Census Bureau. The data itself is governed by the terms of use provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html).\n",
    "\n",
    "\n",
    "__ML Problem:__ \n",
    "Ultimately, the goal will be to predict whether an individual's income is above \\\\$50,000. We will  filter the ACS PUMS data sample to only include individuals above the age of 16, who reported usual working hours of at least 1 hour per week in the past year, and an income of at least \\\\$100. The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable substitute to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult). The income threshold can be changed easily to define new prediction tasks.\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Data Processing</a>\n",
    "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
    "    * <a href=\"#22\">Select features to build the model</a>\n",
    "    * <a href=\"#23\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#24\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "3. <a href=\"#3\">Train (and Tune) a Classifier</a>\n",
    "4. <a href=\"#4\">Test the Classifier</a>\n",
    "5. <a href=\"#5\">Accuracy Difference and DPPL</a>\n",
    "\n",
    "\n",
    "Before building the logistic regression model, let's have a quick look at how a linear regression can be turned into a classifier.\n",
    "\n",
    "Let's assume we want to use a feature (e.g. Class of Worker) to build a model that can predict the income class. A first step could be to plot the feature vs the model target. As class of worker is a categorical feature we introduce a little jitter to see the data points more easily.\n",
    "\n",
    "<img src=\"img/fig-data.png\" alt=\"drawing\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "Now that we have a plot, we can fit a linear regression through the data points.\n",
    "\n",
    "<img src=\"img/fig-lr.png\" alt=\"drawing\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "As we notice, the linear regression line extends beyond the target values. This is not ideal as we can end up making predictions that are outside the model target range (0 or 1); a linear regression can predict values in the range ($-\\inf$, $+\\inf$). To squish the linear regression values into a range of 0 to 1, we need to use a helper function. For this, we can use the sigmoid function:\n",
    "\n",
    "<center>\n",
    "$\\frac{1}{1 + \\exp(- (w_{0} + w_{1}\\cdot x_{1}+ w_{2}\\cdot x_{2} + ...))}$\n",
    "</center>  \n",
    "<p><br></p>\n",
    "\n",
    "Wrapping the linear regression in a sigmoid function will create a so-called Logistic Regression and allow us to make binary predictions as the resulting values will be in the rage 0-1.\n",
    "\n",
    "<img src=\"img/fig-lg.png\" alt=\"drawing\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "We can simply read out the values on the line now to get a probability for a certain individual obtaining a certain outcome. This covers the basics of Logistic Regression and it's time to code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes an installation of the SageMaker kernel `.conda-mlu-rai:Python` through the `environment.yaml` file in SageMaker Sudio Labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshaping/basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Operational libraries\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Fairness libraries\n",
    "from folktables.acs import *\n",
    "from folktables.folktables import *\n",
    "from folktables.load_acs import *\n",
    "\n",
    "# Jupyter(lab) libraries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To read in the dataset, we will be using [folktables](https://github.com/zykls/folktables) which provides access to the US Census dataset. Folktables contains predefined prediction tasks but also allows the user to specify the problem type.\n",
    "\n",
    "The US Census dataset distinguishes between household and individuals. To obtain data on individuals, we use `ACSDataSource` with `survey=person`. The feature names for the US Census data follow the same distinction and use `P` for `person` and `H` for `household`, e.g.: `AGEP` refers to age of an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_features = [\n",
    "    \"AGEP\",  # age individual\n",
    "    \"COW\",  # class of worker\n",
    "    \"SCHL\",  # educational attainment\n",
    "    \"MAR\",  # marital status\n",
    "    \"OCCP\",  # occupation\n",
    "    \"POBP\",  # place of birth\n",
    "    \"RELP\",  # relationship\n",
    "    \"WKHP\",  # hours worked per week past 12 months\n",
    "    \"SEX\",  # sex\n",
    "    \"RAC1P\",  # recorded detailed race code\n",
    "    \"PWGTP\",  # persons weight\n",
    "    \"GCL\",  # grand parents living with grandchildren\n",
    "]\n",
    "\n",
    "# Define the prediction problem and features\n",
    "ACSIncome = folktables.BasicProblem(\n",
    "    features=income_features,\n",
    "    target=\"PINCP\",  # total persons income\n",
    "    target_transform=lambda x: x > 50000,\n",
    "    group=\"RAC1P\",\n",
    "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    postprocess=lambda x: x,  # applies post processing, e.g. fill all NAs\n",
    ")\n",
    "\n",
    "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "# Specify region (here: California) and load data\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "# Apply transformation as per problem statement above\n",
    "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)\n",
    "\n",
    "# Convert numpy array to dataframe\n",
    "df = pd.DataFrame(\n",
    "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
    "    columns=income_features + [\">50k\"],\n",
    ")\n",
    "\n",
    "# For further modelling we want to use only 2 groups (see DATAPREP notebook for details)\n",
    "df = df[df[\"RAC1P\"].isin([6, 8])].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Data Processing</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We look at number of rows, columns, and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check how many rows and columns we have in the data frame\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that all columns are numerical (`dtype = float64`). However, when checking the column headers (and information at top of the notebook), we should notice that we are actually dealing with multimodal data. We expect to see a mix of categorical, numerical and potentially even text information.\n",
    "\n",
    "Let's cast the features accordingly. We start by creating list for each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"COW\",\n",
    "    \"SCHL\",\n",
    "    \"MAR\",\n",
    "    \"OCCP\",\n",
    "    \"POBP\",\n",
    "    \"RELP\",\n",
    "    \"SEX\",\n",
    "    \"RAC1P\",\n",
    "    \"GCL\",\n",
    "]\n",
    "\n",
    "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We cast categorical features to `category`\n",
    "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
    "\n",
    "# We cast numerical features to `int`\n",
    "df[numerical_features] = df[numerical_features].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with `.info()` again to make sure the changes took effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, so we can now separate model features from model target to explore them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_target = \">50k\"\n",
    "model_features = categorical_features + numerical_features\n",
    "\n",
    "print(\"Model features: \", model_features)\n",
    "print(\"Model target: \", model_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check that that target is not accidentally part of the features\n",
    "model_target in model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good here. We made sure that the target is not in the feature list. If we find the above statement showing `True` we need to remove the target by calling `model_features.remove(model_target)`.\n",
    "\n",
    "Let's have a look at missing values next.\n",
    "\n",
    "\n",
    "#### Missing values\n",
    "The quickest way to check for missing values is to use `.isna().sum()`. This will provide a count of how many missing values we have. In fact, we can also see the count of missing values with `.info()` as it provided a count of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the plots, let's have a look at how many unique instances we have per column. This helps us avoid plotting charts with hundreds of unique values. Let's filter for columns with fewer than 10 unique instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortlist_fts = (\n",
    "    df[model_features]\n",
    "    .apply(lambda col: col.nunique())\n",
    "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(shortlist_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[model_target].value_counts().plot.bar(color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we are dealing with an imbalanced dataset. This means there are more examples for one type of results (here: 0; meaning individuals earning $\\leq$ 50k). This is relevant for model choice and potential up-sampling or down-sampling to balance out the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature distribution(s)\n",
    "\n",
    "Let's now plot bar charts for the shortlist features of our dataset (as per above: shortlist - feature columns with less than 10 unique instance classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "fig.suptitle(\"Feature Bar Plots\")\n",
    "\n",
    "fts = range(len(shortlist_fts.index.tolist()))\n",
    "for i, ax in zip(fts, axs.ravel()):\n",
    "    df[shortlist_fts.index.tolist()[i]].value_counts().plot.bar(color=\"black\", ax=ax)\n",
    "    ax.set_title(shortlist_fts.index.tolist()[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 <a name=\"22\">Select features to build the model</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "During the extended EDA in the DATAPREP notebook, we learned that `GCL` is a feature that is equally present for both outcome types and also contains a lot of missing values. Therefore, we can drop it from the list of features we want to use for model build. We also drop `OCCP` and `POBP` as those features have too many unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_remove = [\"GCL\", \"OCCP\", \"POBP\"]\n",
    "\n",
    "# Drop to_remove features from the respective list(s) - if applicable\n",
    "for ft in to_remove:\n",
    "    if ft in model_features:\n",
    "        model_features.remove(ft)\n",
    "    if ft in categorical_features:\n",
    "        categorical_features.remove(ft)\n",
    "    if ft in numerical_features:\n",
    "        numerical_features.remove(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also clean up the dataframe and only keep the features and columns we need\n",
    "df = df[model_features + [model_target]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 <a name=\"23\">Train - Validation - Test Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "To get a training, test and validation set, we will use sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    df, test_size=0.1, shuffle=True, random_state=23\n",
    ")\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Print the shapes of the Train - Test Datasets\n",
    "print(\n",
    "    \"Train - Test - Validation datasets shapes: \",\n",
    "    train_data.shape,\n",
    "    test_data.shape,\n",
    "    val_data.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 <a name=\"24\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a full model pipeline. We need pre-processing split per data type, and then combine everything back into a composite pipeline along with a model. To achieve this, we will use sklearns `Pipeline` and `ColumnTransformer`.\n",
    "\n",
    "__Step 1 (set up pre-processing per data type):__\n",
    "> For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's `SimpleImputer`, followed by a `MinMaxScaler` (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "\n",
    " > In the categorical features pipeline, the __categorical_processor__ below, we impute with a placeholder value and encode with sklearn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    " \n",
    "__Step 2 (combining pre-processing methods into a transformer):__ \n",
    " > The selective preparations of the dataset features are then put together into a collective `ColumnTransformer`, to be finally used in a Pipeline along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future.\n",
    "   \n",
    "__Step 3 (combining transformer with a model):__ \n",
    "> Combine `ColumnTransformer` from Step 2 with a selected algorithm in a new pipeline. For example, the algorithm could be a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### STEP 1 ###\n",
    "##############\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [(\"num_imputer\", SimpleImputer(strategy=\"mean\")), (\"num_scaler\", MinMaxScaler())]\n",
    ")\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### STEP 2 ###\n",
    "##############\n",
    "\n",
    "# Combine all data preprocessors from above\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### STEP 3 ###\n",
    "##############\n",
    "\n",
    "# Pipeline desired all data transformers, along with an estimator at the end\n",
    "# Later you can set/reach the parameters using the names issued - for hyperparameter tuning, for example\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_processing\", data_processor),\n",
    "        (\"lg\", LogisticRegression(solver=\"lbfgs\", penalty=None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will come in handy especially when building more complex pipelines, stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. <a name=\"3\">Train a Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use the pipeline with the desired data transformers, along with a Logistic Regression estimator for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Training\n",
    "\n",
    "We train the classifier with`.fit()` on our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get train data to train the classifier\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the classifier to the train data\n",
    "# Train data going through the Pipeline is imputed (with means from the train data),\n",
    "#   scaled (with the min/max from the train data),\n",
    "#   and finally used to fit the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't tune the classifier at this point, and simply use the validation set as additional test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get validation data to tune the classifier\n",
    "X_val = val_data[model_features]\n",
    "y_val = val_data[model_target]\n",
    "\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"Model performance on the validation set:\")\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. <a name=\"4\">Test the Classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's now evaluate the performance of the trained classifier on the test dataset. We use `.predict()` this time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get test data to evaluate the performance of the classifier\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted model to make predictions on the test dataset\n",
    "# Test data going through the Pipeline is imputed (with means from the train data),\n",
    "#   scaled (with the min/max from the train data),\n",
    "#   and finally used to make predictions\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Model performance on the test set:\")\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. <a name=\"5\">Accuracy Difference and DPPL</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### DPPL\n",
    "DPPL (Difference in Proportion of Predicted Labels) is an extension of DPL where the outcome we compare is the prediction the model creates (rather than the ground truth value). The equation remains basically the same, only we are counting positive outcomes as per model prediction:\n",
    "\n",
    "\n",
    "$\\large DPPL = \\frac{\\hat{n}_{pred>50k \\wedge RAC1P=6}}{n_{RAC1P=6}} - \\frac{\\hat{n}_{pred>50k \\wedge RAC1P=8}}{n_{RAC1P=8}}$\n",
    "\n",
    "To calculate DPPL more easily, let's write a function for it that can take different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dpl(sensitive_attribute_name, attr_val, target, dataframe):\n",
    "    \"\"\"Function to calculate DPL or DPPL (depending on specified target).\"\"\"\n",
    "    for val in attr_val:\n",
    "        globals()[f\"n_pos_gr{val}\"] = len(\n",
    "            dataframe[\n",
    "                (dataframe[target] == 1) & (dataframe[sensitive_attribute_name] == val)\n",
    "            ]\n",
    "        )\n",
    "        globals()[f\"n_gr{val}\"] = len(\n",
    "            dataframe[dataframe[sensitive_attribute_name] == val]\n",
    "        )\n",
    "\n",
    "    dpl = n_pos_gr6 / n_gr6 - n_pos_gr8 / n_gr8\n",
    "    return dpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataframe that contains predictions and the sensitive attribute\n",
    "dpl_df = pd.concat(\n",
    "    [\n",
    "        test_data.reset_index(drop=True)[[\"RAC1P\", \">50k\"]],\n",
    "        pd.Series(test_predictions, name=\"y_test_pred\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "dpl(\"RAC1P\", [6, 8], \"y_test_pred\", dpl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the original DPL value (the difference in proportion of labels in the original dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dpl(\"RAC1P\", [6, 8], \">50k\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPL value is smaller than the DPPL value. It seems that the model is producing a more biased output compared to the ground truth data. This could be due to the different success rates for the different groups in our model. Generally, models will make better predictions for the larger group (meaning, the larger group drives the overall model performance and whatever outcome is dominant in the larger group, will occur more frequently in the predictions).\n",
    "\n",
    "Let's see if we can observe this behavior in our model predictions and calculate overall accuracy first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy score across all groups\n",
    "accuracy_score(dpl_df[\">50k\"], dpl_df[\"y_test_pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at the accuracy difference between `RAC1P=6` and `RAC1P=8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy score for RAC1P=1\n",
    "acc_gr6 = accuracy_score(\n",
    "    dpl_df[dpl_df[\"RAC1P\"] == 6][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 6][\"y_test_pred\"]\n",
    ")\n",
    "\n",
    "acc_gr6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy score for RAC1P=8\n",
    "acc_gr8 = accuracy_score(\n",
    "    dpl_df[dpl_df[\"RAC1P\"] == 8][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 8][\"y_test_pred\"]\n",
    ")\n",
    "\n",
    "acc_gr8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Difference\n",
    "We calculate the Accuracy Difference by subtracting the two values we calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_gr6 - acc_gr8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuracy for the group we assume to be at an advantage is higher than the accuracy for the disfavored group. We can dive deeper to see what's going on and plot the confusion matrix and we should also look at confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for RAC1P=8\n",
    "confusion_matrix(\n",
    "    dpl_df[dpl_df[\"RAC1P\"] == 8][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 8][\"y_test_pred\"]\n",
    ").ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this confusion matrix with the one for the favored group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for RAC1P=1\n",
    "confusion_matrix(\n",
    "    dpl_df[dpl_df[\"RAC1P\"] == 6][\">50k\"], dpl_df[dpl_df[\"RAC1P\"] == 6][\"y_test_pred\"]\n",
    ").ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create confidence intervals, we need to create samples first. We can then repeatedly calculate the metric that interests us from the new sample and eventually summarize in aggregate (or visualize in e.g., box plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling(n_iter=10, group=1):\n",
    "    tn_ls, fp_ls, fn_ls, tp_ls = [], [], [], []\n",
    "    output = {}\n",
    "    for i in range(0, n_iter):\n",
    "        sample = resample(test_data, replace=True, n_samples=5551, stratify=None)\n",
    "        sample = sample[sample[\"RAC1P\"] == group]\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            sample[model_target], pipeline.predict(sample[model_features])\n",
    "        ).ravel()\n",
    "        tn_ls.append(tn), fp_ls.append(fp), fn_ls.append(fn), tp_ls.append(tp)\n",
    "    output[\"tn\"] = tn_ls\n",
    "    output[\"fp\"] = fp_ls\n",
    "    output[\"fn\"] = fn_ls\n",
    "    output[\"tp\"] = tp_ls\n",
    "    return pd.DataFrame.from_dict(output, orient=\"columns\", dtype=None, columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Classification Errors\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Group 6\")\n",
    "ax2.title.set_text(\"Group 8\")\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(data=sampling(n_iter=100, group=6), ax=ax1)\n",
    "sns.boxplot(data=sampling(n_iter=100, group=8), ax=ax2)\n",
    "\n",
    "# Align y-axis\n",
    "ax1.sharey(ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we can see that group 6 has a much higher share of positive predictions than group 8 (add FP + TP for total positive predictions). Furthermore, we observe that the errors are not balanced between false positives and false negatives for the disfavored group (whereas they are within the error bounds for the favored group). For group 8 there is a larger share of false negatives (predicted salary as $\\leq$ 50k, when it was actually higher) than false positives. \n",
    "This is concerning:\n",
    "- The model is penalizing individuals that should have been predicted as positive $\\rightarrow$ if this model is used to show job ads based on a 50k salary threshold, a lot of individuals that should have been eligible to see the ad, won't receive it. This causes a negative reinforcement of existing bias.\n",
    "- The imbalance is not as drastic for the favored group. For this group, we observe more false positives, than false negatives. This means that people are incorrectly predicted positive for the favored group. This causes further amplification of the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Initialize figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Set title of figure\n",
    "fig.suptitle(\"Comparison of Model Predictions and Baseline\")\n",
    "\n",
    "# Set title\n",
    "ax1.title.set_text(\"Baseline target distribution\")\n",
    "ax2.title.set_text(\"Prediction distribution\")\n",
    "\n",
    "# Create plots\n",
    "dpl_df.groupby([\"RAC1P\", model_target]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax1\n",
    ")\n",
    "dpl_df.groupby([\"RAC1P\", \"y_test_pred\"]).size().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, color=sns.husl_palette(2), ax=ax2\n",
    ")\n",
    "# Align y-axis\n",
    "ax2.sharey(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the predictions are less favorable for the disfavored group with even more '0' outcomes than the baseline (which itself was already very biased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-mlu-rai:Python",
   "language": "python",
   "name": "conda-env-.conda-mlu-rai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
