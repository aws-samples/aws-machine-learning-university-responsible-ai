{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Responsible AI - Exploratory Data Analysis</a>\n",
    "\n",
    "This notebook shows how to quantify and visualize correlations (scatter plots, correlation matrix) and generate descriptive statistics (histograms and bar plots). To measure bias before training a model, we will use $CI_{norm}$ and $DPL$. These are just example measure that can be used pre-training. Make sure to try more measures when building a model.\n",
    "\n",
    "__Dataset:__ \n",
    "You will download a dataset for this exercise using [folktables](https://github.com/zykls/folktables). Folktables provides an API to download data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) files which are managed by the US Census Bureau. The data itself is governed by the terms of use provided by the Census Bureau. For more information, see the [Terms of Service](https://www.census.gov/data/developers/about/terms-of-service.html).\n",
    "\n",
    "\n",
    "__ML Problem:__ \n",
    "Ultimately, the goal will be to predict whether an individual's income is above \\\\$50,000. We will  filter the ACS PUMS data sample to only include individuals above the age of 16, who reported usual working hours of at least 1 hour per week in the past year, and an income of at least \\\\$100. The threshold of \\\\$50,000 was chosen so that this dataset can serve as a comparable substitute to the [UCI Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult). The income threshold can be changed easily to define new prediction tasks.\n",
    "\n",
    "__Table of contents__\n",
    "\n",
    "1. <a href=\"#1\">Loading Data</a>\n",
    "2. <a href=\"#2\">Data Overview</a>\n",
    "3. <a href=\"#3\">Bar Plots \\& Histograms</a>\n",
    "4. <a href=\"#4\">Scatter Plots</a>\n",
    "5. <a href=\"#5\">Correlation Matrix</a>\n",
    "5. <a href=\"#6\">$CI_{norm}$ and $DPL$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes an installation of the SageMaker kernel `.conda-mlu-rai:Python` through the `environment.yaml` file in SageMaker Sudio Labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshaping/basic libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "# Operational libraries\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Fairness libraries\n",
    "from folktables.acs import *\n",
    "from folktables.folktables import *\n",
    "from folktables.load_acs import *\n",
    "\n",
    "# Jupyter(lab) libraries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Loading Data</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the dataset, we will be using [folktables](https://github.com/zykls/folktables) which provides access to the US Census dataset. Folktables contains predefined prediction tasks but also allows the user to specify the problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Census dataset distinguishes between household and individuals. To obtain data on individuals, we use `ACSDataSource` with `survey=person`. The feature names for the US Census data follow the same distinction and use `P` for `person` and `H` for `household`, e.g.: `AGEP` refers to age of an individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "income_features = [\n",
    "    \"AGEP\",  # age individual\n",
    "    \"COW\",  # class of worker\n",
    "    \"SCHL\",  # educational attainment\n",
    "    \"MAR\",  # marital status\n",
    "    \"OCCP\",  # occupation\n",
    "    \"POBP\",  # place of birth\n",
    "    \"RELP\",  # relationship\n",
    "    \"WKHP\",  # hours worked per week past 12 months\n",
    "    \"SEX\",  # sex\n",
    "    \"RAC1P\",  # recorded detailed race code\n",
    "    \"PWGTP\",  # persons weight\n",
    "    \"GCL\",  # grandparents living with grandchildren\n",
    "    \"SCH\",  # school enrollment\n",
    "]\n",
    "\n",
    "# Define the prediction problem and features\n",
    "ACSIncome = folktables.BasicProblem(\n",
    "    features=income_features,\n",
    "    target=\"PINCP\",  # total persons income\n",
    "    target_transform=lambda x: x > 50000,\n",
    "    group=\"RAC1P\",\n",
    "    preprocess=adult_filter,  # applies the following conditions; ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    postprocess=lambda x: x,  # applies post processing, e.g. fill all NAs\n",
    ")\n",
    "\n",
    "# Initialize year, duration (\"1-Year\" or \"5-Year\") and granularity (household or person)\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "# Specify region (here: California) and load data\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "# Apply transformation as per problem statement above\n",
    "ca_features, ca_labels, ca_group = ACSIncome.df_to_numpy(ca_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Data Overview</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We want to go through basic steps of exploratory data analysis (EDA), performing initial data investigations to discover patterns, spot anomalies, and look for insights to inform later ML modeling choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert numpy array to dataframe\n",
    "df = pd.DataFrame(\n",
    "    np.concatenate((ca_features, ca_labels.reshape(-1, 1)), axis=1),\n",
    "    columns=income_features + [\">50k\"],\n",
    ")\n",
    "\n",
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check how many rows and columns we have in the data frame\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Casting data\n",
    "We can clearly see that all columns are numerical (`dtype = float64`). However, when checking the column headers (and information at top of the notebook), we should notice that we are actually dealing with multimodal data. We expect to see a mix of categorical, numerical and potentially even text information.\n",
    "\n",
    "Let's cast the features accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"COW\",\n",
    "    \"SCHL\",\n",
    "    \"MAR\",\n",
    "    \"OCCP\",\n",
    "    \"POBP\",\n",
    "    \"RELP\",\n",
    "    \"SEX\",\n",
    "    \"RAC1P\",\n",
    "    \"GCL\",\n",
    "    \"SCH\",\n",
    "]\n",
    "\n",
    "numerical_features = [\"AGEP\", \"WKHP\", \"PWGTP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing you might notice here are the unintuitive feature names; with a few exceptions it is very difficult to understand what the features refer to; for example `MAR` could refer to March. In this dataset, `MAR` is actually encoding the marital status. It could be worthwhile to rename the features, so it will be easier later to understand what they actually refer to. This can be done by creating a renaming dictionary and by using the Pandas `.rename()` method.\n",
    "\n",
    "```\n",
    "# Create dictionary for new column names\n",
    "rename_dict = {\"COW\":\"worker_class\", \"SCHL\":\"educational_attainment\"} \n",
    "\n",
    "# Apply new names to dataframe\n",
    "df.rename(rename_dict, axis=1, inplace=True)\n",
    "```\n",
    "For now, you can skip this and proceed with the next step of casting the features according to their true data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We cast categorical features to `category`\n",
    "df[categorical_features] = df[categorical_features].astype(\"object\")\n",
    "\n",
    "# We cast numerical features to `int`\n",
    "df[numerical_features] = df[numerical_features].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with `.info()` again to make sure the changes took effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, so we can now separate model features from model target to explore them separately. \n",
    "\n",
    "### 2.2. Model Target vs. Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_target = \">50k\"\n",
    "model_features = categorical_features + numerical_features\n",
    "\n",
    "print(\"Model features: \", model_features)\n",
    "print(\"Model target: \", model_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check that that target is not accidentally part of the features\n",
    "model_target in model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks good. You made sure that the target is not in the feature list. If you find the above statement showing `True` you need to remove the target by calling `model_features.remove(model_target)`. Next, you want to start creating some visualizations of the data that you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Unique instances\n",
    "Before starting with the plots, let's have a look at how many unique instances we have per column. This helps us avoid plotting charts with hundreds of unique values. Let's filter for columns with fewer than 10 unique categories. Those are the ones we want to plot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortlist_fts = (\n",
    "    df[model_features]\n",
    "    .apply(lambda col: col.nunique())\n",
    "    .where(df[model_features].apply(lambda col: col.nunique()) < 10)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(shortlist_fts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Bar Plots \\& Histograms</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section, you examine the data with plots. \n",
    "\n",
    "- __Important note:__ These plots ignore null (missing) values. As you saw in the section above, there is only one column with `NA` values in our dataset: GCL (grandparents living with grandchildren).\n",
    "\n",
    "For plotting you need to distinguish between plots for categorical data (bar plots) and plots for numerical data (histograms and scatter plots).\n",
    "\n",
    ">__Bar plots__: These plots show counts of categorical data fields. The `value_counts()` function yields the counts of each unique value. It is useful for categorical variables. To turn the value count into a plot, simply add `.plot.bar()`.\n",
    "\n",
    ">__Histograms:__ Histograms show distributions of numeric data. Data is divided into \"buckets\" or \"bins\". You use histograms for numerical data to group datapoints together into buckets. The command to create a histogram with 5 buckets is `df[feature_name].plot.hist(bins=5)`.\n",
    "\n",
    "### 3.1. Bar plots\n",
    "The features that are eligible for bar plots are all categorical features and also the target feature (which is a binary categorical value). First, let's look at the distribution of the model target.\n",
    "\n",
    "#### 3.1.1. Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[model_target].value_counts().plot.bar(color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that we eventually want to build a model that can consider different groups. The sensitive attribute for this example will be the `RAC1P` feature. Let's plot a similar chart but include another dimension. To do this, we need to group by the feature we want to encode. We also want to stack the bar chart to be able to compare to the bar chart above. We will set the `alpha` transparency value below 1 to better see the horizontal grid lines of the plot background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set color map for the different categories\n",
    "cmap = sns.set_palette(\"husl\", 9)\n",
    "\n",
    "# Perform grouping based on target and feature\n",
    "df.groupby([model_target, \"RAC1P\"])[\"RAC1P\"].count().unstack().plot(\n",
    "    kind=\"bar\", stacked=True, alpha=0.8\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=(1.04, 0), title=\"RAC1P\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Feature distribution(s)\n",
    "Let's start exploring other features (so far we only looked at the distribution of the target). The first feature we might want to explore, could be the sensitive attribute itself. It can be helpful to know how many groups we have in a sensitive feature column and how many instances there are per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"RAC1P\"].value_counts().plot.bar(color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the same chart but include another dimension (the outcome). For this, we want to use another library, Seaborn. Seaborn has a method called `countplot()` which allows to pass a dataframe and feature columns as well as an additional column to use for color encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to plot RAC1P on the x-axis and get counts; in addition we color based on model target category\n",
    "sns.countplot(x=\"RAC1P\", hue=model_target, data=df, palette=\"husl\", dodge=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert these bar charts to percentage numbers to compare the groups more directly. Ultimately, you want to deep dive into the groups with the biggest differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by RAC1P feature and count\n",
    "perc_df = pd.DataFrame(df.groupby(\"RAC1P\")[model_target].value_counts()).rename(\n",
    "    {model_target: \"count\"}, axis=1\n",
    ")\n",
    "\n",
    "# Calculate percentage total\n",
    "perc_df[\"count\"] / perc_df.groupby(\"RAC1P\")[\"count\"].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that the outcome is not equally distributed across all `RAC1P` categories. The biggest discrepancy appears to be category 8 - for this group there are approx. 4x as many instances of income $\\leq$ 50k as above 50k. There are 22793 individuals in group 8. If you compare this to group 6 you should notice that the distribution of outcomes is very different there with an almost 50/50 split for the income levels.\n",
    "\n",
    "Let's plot another countplot for a feature where we don't expect to see big differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"GCL\", hue=model_target, data=df, palette=\"husl\", dodge=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of barplot can be created with `.catplot()`. This method allows to specify 3 dimensions of encoding at the same time. We can specify the feature that we want counts for (e.g. count of how many instanced for each job category we have), generate comparison columns using a second feature (here `SEX`) and finally encode outcome as color again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"COW\", col=\"SEX\", kind=\"count\", hue=model_target, data=df, palette=\"husl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above allows us to observe a pay difference by sex and also by work class (job family)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Histograms\n",
    "\n",
    "Histograms show distributions of numeric data. Data is divided into \"buckets\" or \"bins\". We use histograms for numerical data to group datapoints together into buckets. The command to create a histogram is `df[<feature_name>].plot.hist(bins=n)`. Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create 10 bins for analyzing the age feature\n",
    "df[\"AGEP\"].plot.hist(bins=10, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have expected a lognormal or normal distribution (with peak at around 45 yrs) but are actually observing something that looks like a bi-modal distribution with a cut-off at 16. The cut-off occurs because the `adult_filter` was applied when loading in the dataset in the beginning to mimic the UCI adult dataset; the double peak could indicate that there are 2 different sub-populations in the dataset. Once again, you might want to overlay another feature to check this. You can use Seaborn to do this. \n",
    "\n",
    "As you are working with numerical data now, you can to use `.displot()`. There are a lot of things you can specify, for example whether you want percentages, absolute counts, stacking the bars or displaying next to one another. For all the details, take a look at the [seaborn documentation](https://seaborn.pydata.org/generated/seaborn.displot.html). \n",
    "\n",
    "This plotting method provides access to different types of histogram plots: \n",
    "- 'hist' (histrogram; instance counts per bin), \n",
    "- 'kde' (kernel-density-estimation) and \n",
    "- 'ecdf' (empirical-cumulative distributions). \n",
    "\n",
    "For a comparison of the methods, you can have a look at the [seaborn documentation](https://seaborn.pydata.org/tutorial/distributions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    df,\n",
    "    x=\"AGEP\",\n",
    "    hue=\"SEX\",\n",
    "    aspect=1.2,\n",
    "    kind=\"hist\",\n",
    "    stat=\"count\",\n",
    "    bins=10,\n",
    "    multiple=\"stack\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is not entirely conclusive; we can observe similar patters for both types of sex that are recorded in the data. This gives rise to the question whether there is a combination of attributes that leads to the bi-modal peaks we can observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also bin the age feature and split the plot by the model target. We would expect the younger age-groups to have lower salaries as they are more likely to still be in education or working lower salary jobs. To plot this, we can include a `col` parameter in `displot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    df,\n",
    "    x=\"AGEP\",\n",
    "    col=model_target,\n",
    "    bins=10,\n",
    "    palette=sns.color_palette(\"husl\", 2),\n",
    "    hue=model_target,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is interesting as it suggest that we have different underlying age distributions for different outcome classes. We can look at this again by using the kernel density estimate option for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    df,\n",
    "    x=\"AGEP\",\n",
    "    hue=model_target,\n",
    "    aspect=1.2,\n",
    "    kind=\"kde\",\n",
    "    palette=sns.color_palette(\"husl\", 2),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Scatter Plots</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Scatter plots are simple 2D plots of two numerical variables that can be used to examine the relationship between two numerical variables. If one variable is moving up and so is the other, it is a so-called positive correlation. If one variable moves down and the other moves up, it is a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    x=\"AGEP\",\n",
    "    y=\"WKHP\",\n",
    "    data=df.sample(\n",
    "        5000, random_state=1\n",
    "    ),  # we take a sample of data for quicker plotting\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this plot, there is not really any particular relationship. Let's try to overlay the outcome with a sensitive attribute by specifying a `hue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df.sample(\n",
    "        5000, random_state=1\n",
    "    ),  # we take a sample of data for quicker plotting\n",
    "    x=\"AGEP\",\n",
    "    y=\"WKHP\",\n",
    "    hue=\"SEX\",\n",
    "    palette=sns.color_palette(\"husl\", 2),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to see a pattern for the different `SEX` attributes with class 1 showing up in the top half of the chart. The original goal was to look at the data split by `RAC1P` and you are now seeing that there is a second pattern emerging for another attribute (`SEX`). You can generally expect these effects to intersect; leading to amplified adverse effects for sub-populations with multiple sensitive attributes. Ultimately, you want to build ML models that also work for all sub-groups and this plot highlights why it is important to check whether there are multiple sensitive attributes in the data that are related to the outcome. In this particular case, you expect a correlation between hours worked and income, so let's quantify this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Correlation Matrix</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Similar to scatter plots, the correlation matrix aims to pinpoint relationships between numerical features. Correlation values of -1 means perfect negative correlation, 1 means perfect positive correlation, and 0 means there is no relationship between the two numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title(\"Correlation Matrix\", fontdict={\"fontsize\": 12}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, you see a positive correlation between age and salary as well as between hours worked and salary. However, does this also hold true if you split by sensitive feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 12))\n",
    "fig.suptitle(\"Correlation Matrix by RAC1P class\")\n",
    "\n",
    "tickers = sorted(df[\"RAC1P\"].unique())\n",
    "for i, ax in zip(tickers, axs.ravel()):\n",
    "    sns.heatmap(\n",
    "        df[df[\"RAC1P\"] == i].corr(),\n",
    "        ax=ax,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        annot=True,\n",
    "    )\n",
    "    ax.set_title(\"Group Nr. %s\" % str(int(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, age is less correlated with the outcome for certain groups. This could indicate, that being a member of a particular group has a bigger influence on the outcome than the age feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Difference in Proportion of Labels (DPL) and $CI_{norm}$</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section, you will have a look at some specific measures that can help us identify bias in the dataset.\n",
    "\n",
    "### Difference in Proportion of Labels\n",
    "The difference in proportions of labels (DPL) compares the proportion of observed outcomes with positive labels for different groups in a dataset. More details about DPL can be found in the [SageMaker Clarify](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-true-label-imbalance.html) documentation.\n",
    "\n",
    "$\\large DPL = \\frac{n_{>50k \\wedge RAC1P=6}}{n_{RAC1P=6}} - \\frac{n_{>50k \\wedge RAC1P=8}}{n_{RAC1P=8}}$\n",
    "\n",
    "To calculate DPL you need to select 2 groups that you want to compare. From the analysis in the beginning, you know that for the feature `RAC1P` group 6 and group 8 have the biggest relative difference in outcome, so you want to use those groups for further analysis. It is possible to calculate DPL for $> 2$ groups by selecting one reference group and then comparing against that particular group. It is also possible to use multiple attributes to establish group membership (`RAC1P` and `SEX`).\n",
    "\n",
    "You can slice dataframe using logical conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_50k_gr6 = len(df[(df[\">50k\"] == 1) & (df[\"RAC1P\"] == 6)])\n",
    "n_gr6 = len(df[df[\"RAC1P\"] == 6])\n",
    "\n",
    "n_50k_gr8 = len(df[(df[\">50k\"] == 1) & (df[\"RAC1P\"] == 8)])\n",
    "n_gr8 = len(df[df[\"RAC1P\"] == 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate $DPL$. For binary and multi-categorical outcomes, the DPL values range over the interval (-1, 1). \n",
    "\n",
    "- Positive DPL values indicate that having attribute $a$ (e.g. group membership \"6\") has a higher proportion of positive outcomes when compared with not having attribute $a$ (e.g. group membership \"8\").\n",
    "\n",
    "- Values of DPL near zero indicate a more equal proportion of positive outcomes between groups with different attributes\n",
    "\n",
    "- Negative DPL values indicate that not having attribute $a$ has a higher proportion of positive outcomes when compared with having attribute $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpl = n_50k_gr6 / n_gr6 - n_50k_gr8 / n_gr8\n",
    "dpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You observe DPL of $0.28$. This means members of group 6 has a higher proportion of positive outcomes.\n",
    "\n",
    "### Class Imbalance (normalized)\n",
    "Class imbalance (CI) occurs when there are different group sizes present in a dataset (groups based on sensitive attribute(s)). Here, you don't consider labels/outcomes and are focusing on the group sizes.\n",
    "\n",
    "$\\large CI_{norm} = \\frac{n_{RAC1P=6}-n_{RAC1P=8}} {n_{RAC1P=6}+n_{RAC1P=8}}$\n",
    "\n",
    "CI values range from -1 to +1.\n",
    "- Positive CI values indicate that the group with attribute $a$ contains more examples than the other group\n",
    "- CI values near 0 indicate that the groups are similar sizes\n",
    "- Negative CI values indicate that the group with attribute $a$ contains fewer examples than the other group\n",
    "\n",
    "Once again, you can use dataframe slicing to calculate the values. In fact, you can use `n_gr6` and `n_gr8` from the $DPL$ calculation as these are the counts you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_norm = (n_gr6 - n_gr8) / (n_gr6 + n_gr8)\n",
    "ci_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of group size imbalance, you can see from the CI metric that there are more examples in our dataset from group 6 than 8. This will be very important to remember for next steps and also for model selection. You will have to specify that the model target is imbalanced and also the groups you want to consider are different sizes. Keep in mind that generally models improve the more data you can provide. The same holds true for group-specific performance. You are dealing with target imbalance and group imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-mlu-rai:Python",
   "language": "python",
   "name": "conda-env-.conda-mlu-rai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
